{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fM7yJpPoZrzF"},"outputs":[],"source":["!pip uninstall -y numpy numba tensorflow torch torchvision torchaudio torchdata torchtext dopamine-rl fastai librosa"]},{"cell_type":"markdown","metadata":{"id":"kRGWKaudsoSu"},"source":["# Instructions\n","This notebook allows you to load and evaluate a huggingface model on a subset of BLiMP (a linguistic acceptability judgment dataset) and GLUE (a natural language understanding benchmark collection). It is HIGHLY recommended to clone the GitHub repository and evaluate your model in the command-line; this will give you more freedom in the kinds of models you can evaluate. However, Colab provides a GPU that will allow you to load and evaluate smaller models.\n","\n","To use this notebook:\n","\n","1. Start by making a copy of this notebook so that you can make edits and run the code: File > Save a copy in Drive.\n","\n","2. Set Runtime > Change runtime type > Hardware accelerator to GPU if it isn't already.\n","\n","3. Run the setup script to install the required packages for evaluating.\n","\n","4. Upload your model to the colab in the `/content/model_folder/` directory. This folder should include the following files, and probably a couple more depending on the type of model and tokenizer you use:\n","* `config.json`\n","* `pytorch_model.bin`\n","* `tokenizer_config.json`\n","* `vocab.json`\n","\n","  a. To obtain these files given your pre-trained model and your tokenizer, load them using huggingface `transformers` and save them using these commands:\n","```\n","tokenizer.save_pretrained(\"./model_dir\")\n","model.save_pretrained(\"./model_dir\")\n","```\n","  b. Then, upload all the contents of `model_dir` (including any other files not mentioned above) to the `model_folder` folder in this Colab.\n","\n","5. Choose the proper model type in the dropdown in the \"load model and evaluate\" cell. Use \"decoder\" for autoregressive (sometimes called \"causal\") language models, like GPT/OPT; \"encoder\" for masked language models, like BERT/RoBERTa; or \"encoder-decoder\" for text-to-text models, like T5/BART.\n","\n","6. Run the cells below to load and evaluate your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lk4Epwozk_sf"},"outputs":[],"source":["#@title Setup script { display-mode: \"form\" }\n","#@markdown Run this cell to install the necessary packages (may take a few minutes).\n","%%shell\n","# Remove previous installation if it exists\n","cd /content\n","mkdir model_folder\n","pip uninstall -y lm-eval\n","rm -rf evaluation-pipeline/\n","\n","# Install evaluation-pipeline\n","git clone -b colab https://github.com/babylm/evaluation-pipeline &> /dev/null\n","cd evaluation-pipeline/\n","pip install -e \".[colab]\"\n","# Install other necessary packages\n","pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 'numpy>=1.24.0' lm_eval --extra-index-url https://download.pytorch.org/whl/cu113\n","\n","# Unpack dataset\n","unzip filter_data.zip"]},{"cell_type":"markdown","metadata":{"id":"FKJO-FU2iXvp"},"source":["**Restart runtime and continue from here!**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTZKbIZbUcfu","outputId":"d91f4c79-1b8a-4f1f-abb3-7a715deb583c","executionInfo":{"status":"ok","timestamp":1690558416931,"user_tz":-180,"elapsed":18178,"user":{"displayName":"Akasl By","userId":"13638724939875132689"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"4iu0RtSOmFGq"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4ftRocjmIa4"},"outputs":[],"source":["#@title Load model and evaluate (BLiMP) { display-mode: \"form\" }\n","model = \"model_dir\" #@param {\"type\": \"string\"}\n","model_type = \"encoder\" #@param [\"decoder\", \"encoder\", \"encoder-decoder\"]\n","# file_name = \"examples3.csv\" #@param {\"type\": \"string\"}\n","# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\"] #@param {\"type\": \"raw\"}\n","\n","%cd /content/evaluation-pipeline\n","%run /content/evaluation-pipeline/babylm_eval.py \\\n","  \"$model\" \\\n","  \"$model_type\" \\\n","  -t \"blimp\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PorzVcsqTrbf"},"outputs":[],"source":["#@title Load model and evaluate ((Super)GLUE) { display-mode: \"form\" }\n","#@markdown Run this cell to fine-tune your model on (Super)GLUE tasks.\n","#@markdown We provide some default hyperparameters that you may adjust.\n","model = \"model_dir\" #@param {\"type\": \"string\"}\n","learning_rate = 5e-5 #@param {\"type\": \"number\"}\n","batch_size = 64 #@param {\"type\": \"integer\"}\n","eval_every = 200 #@param {\"type\": \"integer\"}\n","patience = 10 #@param {\"type\": \"integer\"}\n","max_epochs = 10 #@param {\"type\": \"integer\"}\n","seed = 12 #@param {\"type\": \"integer\"}\n","# file_name = \"examples3.csv\" #@param {\"type\": \"string\"}\n","# model_names = [\"opt-125m\", \"opt-350m\", \"opt-1.3b\", \"opt-2.7b\"] #@param {\"type\": \"raw\"}\n","\n","%cd /content/evaluation-pipeline\n","!./finetune_all_tasks.sh \\\n","    \"$model\" \\\n","    \"$learning_rate\" \\\n","    \"$patience\" \\\n","    \"$batch_size\" \\\n","    \"$eval_every\" \\\n","    \"$max_epochs\" \\\n","    \"$seed\""]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}