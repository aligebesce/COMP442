{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1GZHYH0QZzxS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695484707760,"user_tz":-180,"elapsed":23360,"user":{"displayName":"Akasl By","userId":"13638724939875132689"}},"outputId":"1cca3bce-dc79-4f62-c7b3-cfa7ee13bf83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sD-_hxQvLQcc"},"outputs":[],"source":["!pip install Transformers\n","!pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ljmozsw-Kumg","executionInfo":{"status":"ok","timestamp":1695484732328,"user_tz":-180,"elapsed":8403,"user":{"displayName":"Akasl By","userId":"13638724939875132689"}}},"outputs":[],"source":["from accelerate import Accelerator\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from transformers import DataCollatorForLanguageModeling, TrainingArguments, LineByLineTextDataset, Trainer\n","from glob import glob\n","import random\n","import os\n","import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DHQ_x2YSc7th","executionInfo":{"status":"ok","timestamp":1695484732329,"user_tz":-180,"elapsed":7,"user":{"displayName":"Akasl By","userId":"13638724939875132689"}}},"outputs":[],"source":["# Define the paths\n","PROJECT_PATH = '/content/drive/MyDrive/BabyLM_Final'\n","MODELS_PATH = os.path.join(PROJECT_PATH, 'model_folders')\n","TRAIN_DATA_PATH = os.path.join(PROJECT_PATH, 'train_data')\n","\n","# Define the curriculums and levels\n","CURRICULUMS = ['C1', 'C2', 'C3', 'C4']\n","LEVELS = ['5-LEVEL', '10-LEVEL', '20-LEVEL']\n","STRATEGIES = ['E2H', 'H2E', 'RND']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5IfdlFRdAcY"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"babylm/roberta-base-strict-small\")\n","config = AutoModelForMaskedLM.from_pretrained(\"babylm/roberta-base-strict-small\").config\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiGnNQDrjJaK"},"outputs":[],"source":["for curriculum in CURRICULUMS:\n","    for level in LEVELS:\n","        # Get all training files for the current curriculum and level\n","        training_data_files = sorted(glob(os.path.join(TRAIN_DATA_PATH, curriculum, level, \"*.train\")))\n","\n","        for strategy in STRATEGIES:\n","            print(f\"Training on curriculum {curriculum}, level {level}, strategy {strategy} data\")\n","            # Construct the output directory for the current curriculum, level, and strategy\n","            output_dir = os.path.join(MODELS_PATH, curriculum, level, strategy)\n","            os.makedirs(output_dir, exist_ok=True)  # Create the directory if it does not exist\n","\n","            # Initialize the model for the current strategy\n","            model = AutoModelForMaskedLM.from_config(config)\n","            training_args = TrainingArguments(output_dir=output_dir, per_device_train_batch_size=64, num_train_epochs=1,)\n","\n","            # Determine the order of the files based on the strategy\n","            if strategy == 'E2H':\n","                files = sorted(training_data_files)\n","            elif strategy == 'H2E':\n","                files = sorted(training_data_files, reverse=True)\n","            elif strategy == 'RND':\n","                files = list(training_data_files)\n","                random.shuffle(files)\n","\n","            # Train the model using curriculum learning on each file in the current level and strategy\n","            for idx, file_path in enumerate(files, start=1):\n","                print(f\"Training on file {idx} in level {level}, curriculum {curriculum}, strategy {strategy}\")\n","\n","                # Prepare dataset for the current file\n","                dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=128)\n","\n","                # Set up Trainer for the current file with the model instance\n","                trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset,)\n","\n","                # Train the model on the current file\n","                trainer.train()\n","\n","            # Save the model AFTER training on all files in the current curriculum, level, and strategy\n","            trainer.save_model(output_dir)\n","            del model\n","            del trainer\n","            torch.cuda.empty_cache()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1eY4STcYuNb5j7BSlhMsa8V0d6n-2ldly","timestamp":1690479847742}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}